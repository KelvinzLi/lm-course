{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attention Mechanism Demo\n",
        "\n",
        "This notebook demonstrates how attention works in practice with visual examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup course imports (choose one approach)\n",
        "\n",
        "# APPROACH 1: If you installed the course as a package (recommended)\n",
        "# pip install -e .  # (run this once from course root)\n",
        "try:\n",
        "    from lmcourse.utils import plot_attention_heatmap\n",
        "    print(\"✅ Using installed package imports\")\n",
        "except ImportError:\n",
        "    # APPROACH 2: Fallback to path manipulation\n",
        "    import sys\n",
        "    from pathlib import Path\n",
        "    \n",
        "    # Add course root to path\n",
        "    course_root = Path().cwd().parent.parent.parent\n",
        "    if str(course_root) not in sys.path:\n",
        "        sys.path.append(str(course_root))\n",
        "    \n",
        "    from lmcourse.utils import plot_attention_heatmap\n",
        "    print(\"✅ Using path-based imports\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_attention(query, key, value):\n",
        "    \"\"\"Simple scaled dot-product attention.\"\"\"\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "    d_k = key.size(-1)\n",
        "    scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "    return output, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample data\n",
        "seq_len = 5\n",
        "d_model = 4\n",
        "\n",
        "# Random query, key, value matrices\n",
        "query = torch.randn(1, seq_len, d_model)\n",
        "key = torch.randn(1, seq_len, d_model) \n",
        "value = torch.randn(1, seq_len, d_model)\n",
        "\n",
        "# Apply attention\n",
        "output, weights = simple_attention(query, key, value)\n",
        "\n",
        "print(f\"Input shapes:\")\n",
        "print(f\"Query: {query.shape}\")\n",
        "print(f\"Key: {key.shape}\")\n",
        "print(f\"Value: {value.shape}\")\n",
        "print(f\"\\nOutput shapes:\")\n",
        "print(f\"Output: {output.shape}\")\n",
        "print(f\"Attention weights: {weights.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize using course utilities\n",
        "plot_attention_heatmap(\n",
        "    weights, \n",
        "    title=\"Attention Weights Example\",\n",
        "    figsize=(8, 6)\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
